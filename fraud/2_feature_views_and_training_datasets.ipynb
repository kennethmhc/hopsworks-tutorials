{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Creation\n",
    "\n",
    "In this notebook, we will create the actual dataset that we will train our model on. In particular, we will:\n",
    "1. Select the features we want to train our model on.\n",
    "2. Specify how the features should be preprocessed.\n",
    "3. Create a dataset split for training and validation data.\n",
    "\n",
    "![tutorial-flow](images/create_training_dataset.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected. Call `.close()` to terminate connection gracefully.\n"
     ]
    }
   ],
   "source": [
    "import hsfs\n",
    "\n",
    "conn = hsfs.connection()\n",
    "fs = conn.get_feature_store()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection\n",
    "\n",
    "We start by selecting all the features we want to include for model training/inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-31 11:42:41,718 INFO: USE `fraud_batch_online_featurestore`\n",
      "2022-05-31 11:42:42,647 INFO: WITH right_fg0 AS (SELECT *\n",
      "FROM (SELECT `fg1`.`fraud_label` `fraud_label`, `fg1`.`category` `category`, `fg1`.`amount` `amount`, `fg1`.`age_at_transaction` `age_at_transaction`, `fg1`.`days_until_card_expires` `days_until_card_expires`, `fg1`.`loc_delta` `loc_delta`, `fg1`.`cc_num` `join_pk_cc_num`, `fg1`.`datetime` `join_evt_datetime`, `fg0`.`trans_volume_mstd` `trans_volume_mstd`, `fg0`.`trans_volume_mavg` `trans_volume_mavg`, `fg0`.`trans_freq` `trans_freq`, `fg0`.`loc_delta_mavg` `loc_delta_mavg`, RANK() OVER (PARTITION BY `fg1`.`cc_num`, `fg1`.`datetime` ORDER BY `fg0`.`datetime` DESC) pit_rank_hopsworks\n",
      "FROM `fraud_batch_online_featurestore`.`transactions_1` `fg1`\n",
      "INNER JOIN `fraud_batch_online_featurestore`.`transactions_4h_aggs_1` `fg0` ON `fg1`.`cc_num` = `fg0`.`cc_num` AND `fg1`.`datetime` >= `fg0`.`datetime`) NA\n",
      "WHERE `pit_rank_hopsworks` = 1) (SELECT `right_fg0`.`fraud_label` `fraud_label`, `right_fg0`.`category` `category`, `right_fg0`.`amount` `amount`, `right_fg0`.`age_at_transaction` `age_at_transaction`, `right_fg0`.`days_until_card_expires` `days_until_card_expires`, `right_fg0`.`loc_delta` `loc_delta`, `right_fg0`.`trans_volume_mstd` `trans_volume_mstd`, `right_fg0`.`trans_volume_mavg` `trans_volume_mavg`, `right_fg0`.`trans_freq` `trans_freq`, `right_fg0`.`loc_delta_mavg` `loc_delta_mavg`\n",
      "FROM right_fg0)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fraud_label</th>\n",
       "      <th>category</th>\n",
       "      <th>amount</th>\n",
       "      <th>age_at_transaction</th>\n",
       "      <th>days_until_card_expires</th>\n",
       "      <th>loc_delta</th>\n",
       "      <th>trans_volume_mstd</th>\n",
       "      <th>trans_volume_mavg</th>\n",
       "      <th>trans_freq</th>\n",
       "      <th>loc_delta_mavg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Grocery</td>\n",
       "      <td>93.51</td>\n",
       "      <td>25.334094</td>\n",
       "      <td>175.912280</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>93.510</td>\n",
       "      <td>93.510</td>\n",
       "      <td>93.510</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Domestic Transport</td>\n",
       "      <td>65.14</td>\n",
       "      <td>25.335632</td>\n",
       "      <td>175.350486</td>\n",
       "      <td>0.319574</td>\n",
       "      <td>65.140</td>\n",
       "      <td>65.140</td>\n",
       "      <td>65.140</td>\n",
       "      <td>0.319574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>Grocery</td>\n",
       "      <td>0.26</td>\n",
       "      <td>25.336235</td>\n",
       "      <td>175.130347</td>\n",
       "      <td>0.314148</td>\n",
       "      <td>0.260</td>\n",
       "      <td>0.260</td>\n",
       "      <td>0.260</td>\n",
       "      <td>0.314148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>Grocery</td>\n",
       "      <td>1.43</td>\n",
       "      <td>25.336660</td>\n",
       "      <td>174.975058</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.845</td>\n",
       "      <td>0.845</td>\n",
       "      <td>0.845</td>\n",
       "      <td>0.157074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Grocery</td>\n",
       "      <td>19.75</td>\n",
       "      <td>25.344710</td>\n",
       "      <td>172.034664</td>\n",
       "      <td>0.105313</td>\n",
       "      <td>19.750</td>\n",
       "      <td>19.750</td>\n",
       "      <td>19.750</td>\n",
       "      <td>0.105313</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fraud_label            category  amount  age_at_transaction  \\\n",
       "0            0             Grocery   93.51           25.334094   \n",
       "1            0  Domestic Transport   65.14           25.335632   \n",
       "2            0             Grocery    0.26           25.336235   \n",
       "3            0             Grocery    1.43           25.336660   \n",
       "4            0             Grocery   19.75           25.344710   \n",
       "\n",
       "   days_until_card_expires  loc_delta  trans_volume_mstd  trans_volume_mavg  \\\n",
       "0               175.912280   0.000000             93.510             93.510   \n",
       "1               175.350486   0.319574             65.140             65.140   \n",
       "2               175.130347   0.314148              0.260              0.260   \n",
       "3               174.975058   0.000000              0.845              0.845   \n",
       "4               172.034664   0.105313             19.750             19.750   \n",
       "\n",
       "   trans_freq  loc_delta_mavg  \n",
       "0      93.510        0.000000  \n",
       "1      65.140        0.319574  \n",
       "2       0.260        0.314148  \n",
       "3       0.845        0.157074  \n",
       "4      19.750        0.105313  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load feature groups.\n",
    "trans_fg = fs.get_feature_group(\"transactions\", 1)\n",
    "window_aggs_fg = fs.get_feature_group(\"transactions_4h_aggs\", 1)\n",
    "\n",
    "# Select features for training data.\n",
    "ds_query = trans_fg.select([\"fraud_label\", \"category\", \"amount\", \"age_at_transaction\", \"days_until_card_expires\", \"loc_delta\"])\\\n",
    "    .join(window_aggs_fg.select_except([\"cc_num\"]), on=\"cc_num\")\\\n",
    "\n",
    "ds_query.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that we computed the features in `transactions_4h_aggs` using 4-hour aggregates. If we had created multiple feature groups with identical schema for different window lengths, and wanted to include them in the join we would need to include a prefix argument in the join to avoid feature name clash. See the [documentation](https://docs.hopsworks.ai/feature-store-api/latest/generated/api/query_api/#join) for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformation Functions\n",
    "Transformation functions are a mathematical mapping of input data that may be stateful - requiring statistics from the partent feature view (such as number of instances of a category, or mean value of a numerical feature)\n",
    "\n",
    "We will preprocess our data using *min-max scaling* on numerical features and *label encoding* on categorical features. To do this we simply define a mapping between our features and transformation functions. This ensures that transformation functions such as *min-max scaling* are fitted only on the training data (and not the validation/test data), which ensures that there is no data leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load transformation functions.\n",
    "min_max_scaler = fs.get_transformation_function(name=\"min_max_scaler\")\n",
    "label_encoder = fs.get_transformation_function(name=\"label_encoder\")\n",
    "\n",
    "# Map features to transformations.\n",
    "transformation_functions = {\n",
    "    \"category\": label_encoder,\n",
    "    \"amount\": min_max_scaler,\n",
    "    \"trans_volume_mavg\": min_max_scaler,\n",
    "    \"trans_volume_mstd\": min_max_scaler,\n",
    "    \"trans_freq\": min_max_scaler,\n",
    "    \"loc_delta\": min_max_scaler,\n",
    "    \"loc_delta_mavg\": min_max_scaler,\n",
    "    \"age_at_transaction\": min_max_scaler,\n",
    "    \"days_until_card_expires\": min_max_scaler,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature View Creation\n",
    "\n",
    "In Hopsworks, you write features to feature groups (where the features are stored) and you read features from feature views. A feature view is a logical view over features, stored in feature groups, and a feature view typically contains the features used by a specific model. This way, feature views enable features, stored in different feature groups, to be reused across many different models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_view = fs.create_feature_view(\n",
    "    name='transactions_view',\n",
    "    query=ds_query,\n",
    "    label=[\"fraud_label\"],\n",
    "    transformation_functions=transformation_functions\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To view and explore data in the feature view we can retrieve batch data using `get_batch_data()` method "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-31 11:43:37,381 INFO: USE `fraud_batch_online_featurestore`\n",
      "2022-05-31 11:43:38,471 INFO: WITH right_fg0 AS (SELECT *\n",
      "FROM (SELECT `fg1`.`category` `category`, `fg1`.`amount` `amount`, `fg1`.`age_at_transaction` `age_at_transaction`, `fg1`.`days_until_card_expires` `days_until_card_expires`, `fg1`.`loc_delta` `loc_delta`, `fg1`.`cc_num` `join_pk_cc_num`, `fg1`.`datetime` `join_evt_datetime`, `fg0`.`trans_volume_mstd` `trans_volume_mstd`, `fg0`.`trans_volume_mavg` `trans_volume_mavg`, `fg0`.`trans_freq` `trans_freq`, `fg0`.`loc_delta_mavg` `loc_delta_mavg`, RANK() OVER (PARTITION BY `fg1`.`cc_num`, `fg1`.`datetime` ORDER BY `fg0`.`datetime` DESC) pit_rank_hopsworks\n",
      "FROM `fraud_batch_online_featurestore`.`transactions_1` `fg1`\n",
      "INNER JOIN `fraud_batch_online_featurestore`.`transactions_4h_aggs_1` `fg0` ON `fg1`.`cc_num` = `fg0`.`cc_num` AND `fg1`.`datetime` >= `fg0`.`datetime`) NA\n",
      "WHERE `pit_rank_hopsworks` = 1) (SELECT `right_fg0`.`category` `category`, `right_fg0`.`amount` `amount`, `right_fg0`.`age_at_transaction` `age_at_transaction`, `right_fg0`.`days_until_card_expires` `days_until_card_expires`, `right_fg0`.`loc_delta` `loc_delta`, `right_fg0`.`trans_volume_mstd` `trans_volume_mstd`, `right_fg0`.`trans_volume_mavg` `trans_volume_mavg`, `right_fg0`.`trans_freq` `trans_freq`, `right_fg0`.`loc_delta_mavg` `loc_delta_mavg`\n",
      "FROM right_fg0)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>amount</th>\n",
       "      <th>age_at_transaction</th>\n",
       "      <th>days_until_card_expires</th>\n",
       "      <th>loc_delta</th>\n",
       "      <th>trans_volume_mstd</th>\n",
       "      <th>trans_volume_mavg</th>\n",
       "      <th>trans_freq</th>\n",
       "      <th>loc_delta_mavg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Grocery</td>\n",
       "      <td>93.51</td>\n",
       "      <td>25.334094</td>\n",
       "      <td>175.912280</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>93.510</td>\n",
       "      <td>93.510</td>\n",
       "      <td>93.510</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Domestic Transport</td>\n",
       "      <td>65.14</td>\n",
       "      <td>25.335632</td>\n",
       "      <td>175.350486</td>\n",
       "      <td>0.319574</td>\n",
       "      <td>65.140</td>\n",
       "      <td>65.140</td>\n",
       "      <td>65.140</td>\n",
       "      <td>0.319574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Grocery</td>\n",
       "      <td>0.26</td>\n",
       "      <td>25.336235</td>\n",
       "      <td>175.130347</td>\n",
       "      <td>0.314148</td>\n",
       "      <td>0.260</td>\n",
       "      <td>0.260</td>\n",
       "      <td>0.260</td>\n",
       "      <td>0.314148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Grocery</td>\n",
       "      <td>1.43</td>\n",
       "      <td>25.336660</td>\n",
       "      <td>174.975058</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.845</td>\n",
       "      <td>0.845</td>\n",
       "      <td>0.845</td>\n",
       "      <td>0.157074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Grocery</td>\n",
       "      <td>19.75</td>\n",
       "      <td>25.344710</td>\n",
       "      <td>172.034664</td>\n",
       "      <td>0.105313</td>\n",
       "      <td>19.750</td>\n",
       "      <td>19.750</td>\n",
       "      <td>19.750</td>\n",
       "      <td>0.105313</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             category  amount  age_at_transaction  days_until_card_expires  \\\n",
       "0             Grocery   93.51           25.334094               175.912280   \n",
       "1  Domestic Transport   65.14           25.335632               175.350486   \n",
       "2             Grocery    0.26           25.336235               175.130347   \n",
       "3             Grocery    1.43           25.336660               174.975058   \n",
       "4             Grocery   19.75           25.344710               172.034664   \n",
       "\n",
       "   loc_delta  trans_volume_mstd  trans_volume_mavg  trans_freq  loc_delta_mavg  \n",
       "0   0.000000             93.510             93.510      93.510        0.000000  \n",
       "1   0.319574             65.140             65.140      65.140        0.319574  \n",
       "2   0.314148              0.260              0.260       0.260        0.314148  \n",
       "3   0.000000              0.845              0.845       0.845        0.157074  \n",
       "4   0.105313             19.750             19.750      19.750        0.105313  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_view.get_batch_data().head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Dataset Creation\n",
    "\n",
    "In Hopsworks training data is a query where the projection (set of features) is determined by the parent FeatureView with an optional snapshot on disk of the data returned by the query.\n",
    "\n",
    "Training Dataset  may contain splits such as: \n",
    "* Training set - the subset of training data used to train a model.\n",
    "* Validation set - the subset of training data used to evaluate hparams when training a model\n",
    "* Test set - the holdout subset of training data used to evaluate a mode\n",
    "\n",
    "Training dataset is created using `fs.create_training_dataset()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset job started successfully, you can follow the progress at https://hopsworks.glassfish.service.consul:8182/p/129/jobs/named/transactions_view_1_1_create_fv_td_31052022114428/executions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VersionWarning: No version provided for creating training dataset, incremented version to `1`.\n"
     ]
    }
   ],
   "source": [
    "td_random_version, td_job = feature_view.create_training_dataset(\n",
    "    description = 'transactions_dataset_random_splitted',\n",
    "    data_format = 'csv',\n",
    "    splits = {'train': 80, 'validation': 20},\n",
    "    train_split = \"train\",\n",
    "    write_options = {'wait_for_job': True},\n",
    "    coalesce = True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create training datasets based event time filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "date_format = \"%Y-%m-%d %H:%M:%S\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create training dataset from January to February data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = int(float(datetime.strptime(\"2022-01-01 00:00:01\", date_format).timestamp()) * 1000)\n",
    "end_time = int(float(datetime.strptime(\"2022-02-28 23:59:59\", date_format).timestamp()) * 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset job started successfully, you can follow the progress at https://hopsworks.glassfish.service.consul:8182/p/129/jobs/named/transactions_view_1_2_create_fv_td_31052022114615/executions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VersionWarning: No version provided for creating training dataset, incremented version to `2`.\n"
     ]
    }
   ],
   "source": [
    "td_jan_feb_version, td_job = feature_view.create_training_dataset(\n",
    "    description = 'transactions_dataset_jan_feb',\n",
    "    data_format = 'csv',\n",
    "    write_options = {'wait_for_job': True},\n",
    "    coalesce = True,\n",
    "    start_time = start_time,\n",
    "    end_time = end_time,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create training dataset from March data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = int(float(datetime.strptime(\"2022-03-01 00:00:01\", date_format).timestamp()) * 1000)\n",
    "end_time = int(float(datetime.strptime(\"2022-03-31 23:59:59\", date_format).timestamp()) * 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset job started successfully, you can follow the progress at https://hopsworks.glassfish.service.consul:8182/p/129/jobs/named/transactions_view_1_3_create_fv_td_31052022114743/executions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VersionWarning: No version provided for creating training dataset, incremented version to `3`.\n"
     ]
    }
   ],
   "source": [
    "td_mar_version, td_job = feature_view.create_training_dataset(\n",
    "    description = 'transactions_dataset_mar',\n",
    "    data_format = 'csv',\n",
    "    write_options = {'wait_for_job': True},\n",
    "    coalesce = True,\n",
    "    start_time = start_time,\n",
    "    end_time = end_time,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Dataset retreival\n",
    "To retrieve training data from storage (already materialised) or from feature groups direcly we can use `get_training_dataset_splits` or `get_training_dataset` methods. If version is not provided or provided version has not already existed, it creates a new version of training data according to given arguments and returns a dataframe. If version is provided and has already existed, it reads training data from storage or feature groups and returns a dataframe. If split is provided, it reads the specific split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UserWarning: Training dataset splits were defined but no `train_split` (the name of the split that is going to be used for training) was provided. Setting this property to `train`. The statistics of this split will be used for transformation functions.\n",
      "FutureWarning: pyarrow.hdfs.HadoopFileSystem is deprecated as of 2.0.0, please use pyarrow.fs.HadoopFileSystem instead.\n"
     ]
    }
   ],
   "source": [
    "td_version, td_df_random = feature_view.get_training_dataset_splits({'train': 80, 'validation': 20}, start_time=None, end_time=None, version = td_random_version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train':        fraud_label  category        amount  age_at_transaction  \\\n",
       " 0                0         0  0.000000e+00            0.010858   \n",
       " 1                0         0  0.000000e+00            0.047378   \n",
       " 2                0         0  0.000000e+00            0.063759   \n",
       " 3                0         0  0.000000e+00            0.954661   \n",
       " 4                0         0  3.336858e-07            0.364075   \n",
       " ...            ...       ...           ...                 ...   \n",
       " 84904            1         5  4.983598e-03            0.206613   \n",
       " 84905            1         5  7.304049e-03            0.909331   \n",
       " 84906            1         5  1.362873e-02            0.516288   \n",
       " 84907            1         8  1.698461e-04            0.132164   \n",
       " 84908            1         8  4.875150e-04            0.488983   \n",
       " \n",
       "        days_until_card_expires  loc_delta  trans_volume_mstd  \\\n",
       " 0                     0.850452   0.024955       0.000000e+00   \n",
       " 1                     0.943722   0.035718       0.000000e+00   \n",
       " 2                     0.132026   0.000044       0.000000e+00   \n",
       " 3                     0.874834   0.183911       0.000000e+00   \n",
       " 4                     0.664382   0.093095       3.336858e-07   \n",
       " ...                        ...        ...                ...   \n",
       " 84904                 0.228934   0.093580       1.922489e-03   \n",
       " 84905                 0.728989   0.214688       4.702467e-03   \n",
       " 84906                 0.314443   0.086154       5.831589e-03   \n",
       " 84907                 0.816431   0.217638       1.698461e-04   \n",
       " 84908                 0.607004   0.043470       4.875150e-04   \n",
       " \n",
       "        trans_volume_mavg    trans_freq  loc_delta_mavg  \n",
       " 0           0.000000e+00  0.000000e+00        0.026888  \n",
       " 1           0.000000e+00  0.000000e+00        0.038485  \n",
       " 2           0.000000e+00  0.000000e+00        0.000047  \n",
       " 3           0.000000e+00  0.000000e+00        0.198159  \n",
       " 4           3.336858e-07  3.336858e-07        0.100307  \n",
       " ...                  ...           ...             ...  \n",
       " 84904       1.922489e-03  1.922489e-03        0.100203  \n",
       " 84905       4.702467e-03  4.702467e-03        0.204941  \n",
       " 84906       5.831589e-03  5.831589e-03        0.099539  \n",
       " 84907       1.698461e-04  1.698461e-04        0.234498  \n",
       " 84908       4.875150e-04  4.875150e-04        0.046838  \n",
       " \n",
       " [84909 rows x 10 columns],\n",
       " 'validation':        fraud_label  category        amount  age_at_transaction  \\\n",
       " 0                0         0  0.000000e+00            0.340603   \n",
       " 1                0         0  3.336858e-07            0.573938   \n",
       " 2                0         0  3.336858e-07            0.612448   \n",
       " 3                0         0  6.673716e-07            0.343896   \n",
       " 4                0         0  6.673716e-07            0.815824   \n",
       " ...            ...       ...           ...                 ...   \n",
       " 21106            1         4  3.122632e-03            0.448547   \n",
       " 21107            1         4  3.132642e-03            0.482468   \n",
       " 21108            1         5  4.428011e-03            0.922010   \n",
       " 21109            1         5  5.702691e-03            0.206613   \n",
       " 21110            1         8  3.098940e-03            0.266895   \n",
       " \n",
       "        days_until_card_expires  loc_delta  trans_volume_mstd  \\\n",
       " 0                     0.208466   0.211902       0.000000e+00   \n",
       " 1                     0.205076   0.023092       1.330339e-01   \n",
       " 2                     0.278781   0.166681       5.505816e-06   \n",
       " 3                     0.509910   0.103394       6.673716e-07   \n",
       " 4                     0.767884   0.091310       6.673716e-07   \n",
       " ...                        ...        ...                ...   \n",
       " 21106                 0.643785   0.165436       1.399955e-03   \n",
       " 21107                 0.728661   0.180755       1.569258e-03   \n",
       " 21108                 0.287209   0.093682       2.328126e-03   \n",
       " 21109                 0.228933   0.098071       2.155544e-03   \n",
       " 21110                 0.270363   0.097779       3.098940e-03   \n",
       " \n",
       "        trans_volume_mavg    trans_freq  loc_delta_mavg  \n",
       " 0           0.000000e+00  0.000000e+00        0.228318  \n",
       " 1           1.330339e-01  1.330339e-01        0.019678  \n",
       " 2           5.505816e-06  5.505816e-06        0.179575  \n",
       " 3           6.673716e-07  6.673716e-07        0.111404  \n",
       " 4           6.673716e-07  6.673716e-07        0.098383  \n",
       " ...                  ...           ...             ...  \n",
       " 21106       1.399955e-03  1.399955e-03        0.134610  \n",
       " 21107       1.569258e-03  1.569258e-03        0.098269  \n",
       " 21108       2.328126e-03  2.328126e-03        0.073158  \n",
       " 21109       2.155544e-03  2.155544e-03        0.101301  \n",
       " 21110       3.098940e-03  3.098940e-03        0.105353  \n",
       " \n",
       " [21111 rows x 10 columns]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "td_df_random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From feature view we can also retrieve feature vecors from online store for low latency model serving \n",
    "Training data version is required for transformation. Call `feature_view.init_serving(version)` to pass the training dataset version. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_view.init_serving(td_version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve single feature vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_view.get_feature_vector({\"cc_num\": \"4473593503484549\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve batch of feature vectors\n",
    "\n",
    "Here we have to let feature_view that we need to get batch of feature vectors by passing `batch=True` to `init_serving` method  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_view.init_serving(td_version, batch=True)\n",
    "card_ids = [\n",
    "    \"4473593503484549\",\n",
    "    \"4336399961348201\",\n",
    "    \"4219785543443381\",\n",
    "    \"4137709749259770\",\n",
    "    \"4573366597272313\",\n",
    "    \"4929411498746287\",\n",
    "    \"4855787436134696\"    \n",
    "]\n",
    "feature_view.get_feature_vectors(entry={\"cc_num\": card_ids})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next Steps\n",
    "\n",
    "In the next notebook, we will train a model on the dataset we created in this notebook."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}