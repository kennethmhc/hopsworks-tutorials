{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"font-width:bold; font-size: 3rem; color:#1EB182;\"><img src=\"../images/icon102.png\" width=\"38px\"></img> **Hopsworks Feature Store** </span><span style=\"font-width:bold; font-size: 3rem; color:#333;\">- Part 02: Training Data & Feature views</span>\n",
    "\n",
    "<span style=\"font-width:bold; font-size: 1.4rem;\">This is the second part of the quick start series of tutorials about Hopsworks Feature Store. This notebook explains how to read from a feature group and create training dataset within the feature store</span>\n",
    "\n",
    "## **üóíÔ∏è In this notebook we will see how to create a training dataset from the feature groups:** \n",
    "1. **Select the features** we want to train our model on,\n",
    "2. **How the features should be preprocessed,**\n",
    "3. **Create a dataset split** for training and validation data.\n",
    "\n",
    "![tutorial-flow](../images/02_training-dataset.png) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hopsworks\n",
    "\n",
    "project = hopsworks.login()\n",
    "\n",
    "fs = project.get_feature_store()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff5f27;\"> üî™ Feature Selection </span>\n",
    "\n",
    "We start by selecting all the features we want to include for model training/inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load feature groups.\n",
    "trans_fg = fs.get_feature_group('transactions_offline_fg', version=1)\n",
    "window_aggs_fg = fs.get_feature_group('transactions_4h_aggs_offline_fg', version=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features for training data.\n",
    "ds_query = trans_fg.select([\"fraud_label\", \"category\", \"amount\", \"age_at_transaction\", \"days_until_card_expires\", \"loc_delta\"])\\\n",
    "    .join(window_aggs_fg.select_except([\"cc_num\"]))\n",
    "\n",
    "ds_query.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that we computed the features in `transactions_4h_aggs` using 4-hour aggregates. If we had created multiple feature groups with identical schema for different window lengths, and wanted to include them in the join we would need to include a prefix argument in the join to avoid feature name clash. See the [documentation](https://docs.hopsworks.ai/feature-store-api/latest/generated/api/query_api/#join) for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ü§ñ Transformation Functions </span>\n",
    "\n",
    "We will preprocess our data using *min-max scaling* on numerical features and *label encoding* on categorical features. To do this we simply define a mapping between our features and transformation functions. This ensures that transformation functions such as *min-max scaling* are fitted only on the training data (and not the validation/test data), which ensures that there is no data leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load transformation functions.\n",
    "min_max_scaler = fs.get_transformation_function(name=\"min_max_scaler\")\n",
    "label_encoder = fs.get_transformation_function(name=\"label_encoder\")\n",
    "\n",
    "# Map features to transformations.\n",
    "transformation_functions = {\n",
    "    \"category\": label_encoder,\n",
    "    \"amount\": min_max_scaler,\n",
    "    \"trans_volume_mavg\": min_max_scaler,\n",
    "    \"trans_volume_mstd\": min_max_scaler,\n",
    "    \"trans_freq\": min_max_scaler,\n",
    "    \"loc_delta\": min_max_scaler,\n",
    "    \"loc_delta_mavg\": min_max_scaler,\n",
    "    \"age_at_transaction\": min_max_scaler,\n",
    "    \"days_until_card_expires\": min_max_scaler,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff5f27;\"> ‚öôÔ∏è Feature View Creation </span>\n",
    "\n",
    "The Feature Views allows schema in form of a query with filters, define a model target feature/label and additional transformation functions.\n",
    "In order to create a Feature View we may use `fs.create_feature_view()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_view = fs.create_feature_view(\n",
    "    name='transactions_offline_fv',\n",
    "    query=ds_query,\n",
    "    labels=[\"fraud_label\"],\n",
    "    transformation_functions=transformation_functions\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## <span style=\"color:#ff5f27;\"> üèãÔ∏è Training Dataset Creation</span>\n",
    "\n",
    "In Hopsworks training data is a query where the projection (set of features) is determined by the parent FeatureView with an optional snapshot on disk of the data returned by the query.\n",
    "\n",
    "**Training Dataset  may contain splits such as:** \n",
    "* Training set - the subset of training data used to train a model.\n",
    "* Validation set - the subset of training data used to evaluate hparams when training a model\n",
    "* Test set - the holdout subset of training data used to evaluate a mode\n",
    "\n",
    "Training dataset is created using `fs.create_train_validation_test_splits()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "td_version, td_job = feature_view.create_train_validation_test_splits(\n",
    "    description = 'transactions_offline_dataset',\n",
    "    data_format = 'csv',\n",
    "    val_size = 0.2,\n",
    "    test_size = 0.1,\n",
    "    write_options = {'wait_for_job': True},\n",
    "    coalesce = True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff5f27;\"> ü™ù Training Dataset retreival </span>\n",
    "\n",
    "To retrieve training data from storage (already materialised) or from feature groups direcly we can use `get_training_dataset` or `get_train_validation_test_splits` methods. You need to provide the version of the training dataset you want to retrieve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_val, y_val, X_test, y_test = feature_view.get_train_validation_test_splits(td_version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The feature view and training dataset are now visible in the UI\n",
    "\n",
    "![fg-overview](../images/fv_overview.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff5f27;\">‚è≠Ô∏è **Next:** Part 03 </span>\n",
    "\n",
    "In the following notebook, we will train a model on the dataset we created in this notebook and have quick overview of the lineage."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
